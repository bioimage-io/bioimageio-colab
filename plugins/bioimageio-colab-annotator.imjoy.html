<config lang="json">
{
  "name": "BioImage.IO Colab Annotator",
  "type": "iframe",
  "tags": [],
  "ui": "",
  "version": "0.1.0",
  "cover": "",
  "description": "Collaborative Annotator for BioImage.IO with Automated Segmentation",
  "icon": "extension",
  "inputs": null,
  "outputs": null,
  "api_version": "0.1.8",
  "env": "",
  "permissions": [],
  "requirements": [
    "https://cdn.jsdelivr.net/npm/hypha-rpc@0.20.47/dist/hypha-rpc-websocket.min.js",
    "https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js",
    "https://docs.opencv.org/4.5.0/opencv.js"
  ],
  "dependencies": []
}
</config>

<script lang="javascript">
    
const loadSamDecoder = async (modelID) => {
  const modelUrls = {
    // "sam_vit_b": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth",  // TODO: Replace with ONNX decoder model
    "sam_vit_b_lm": "https://uk1s3.embassy.ebi.ac.uk/public-datasets/sam-vit_b_lm-decoder/1/model.onnx",
    // "sam_vit_b_em_organelles": "https://uk1s3.embassy.ebi.ac.uk/public-datasets/bioimage.io/noisy-ox/1/files/vit_b.pt",  // TODO: Replace with ONNX decoder model
  };

  const modelUrl = modelUrls[modelID];
  console.log('Starting to created ONNX model from', modelUrl);
  const modelPromise = ort.InferenceSession.create(modelUrl)
    .then((model) => {
      console.log('ONNX model created', model);
      return model;
    })
    .catch((error) => {
      console.error('Error creating ONNX model:', error);
      throw error; // Propagate the error
    });

  return modelPromise;
};

const getServices = async (ctx) => {
  // Extract configuration settings and check if they are valid
  const config = ctx.config || {};
  config.serverUrl = config.serverUrl || "https://hypha.aicell.io";
  config.token = config.token || null;
  config.imageProviderId = config.imageProviderId || null;
  config.samServiceId = config.samServiceId || "bioimageio-colab/microsam";

  // Connect to the Hypha server
  console.log(`Connecting to server ${config.serverUrl}...`);
  await api.showMessage(`Connecting to server ${config.serverUrl}...`);
  const server = await hyphaWebsocketClient.connectToServer({
    server_url: config.serverUrl,
    token: config.token,
  });

  // Get the current workspace and user ID
  const currentWorkspace = server.config.user.scope.current_workspace;
  const userID = server.config.user.id;
  console.log(`Connected to workspace ${currentWorkspace} as user ${userID}.`);

  let dataService = null;
  let samService = null;

  if (config.imageProviderId) {
    // Get the image provider service from the server
    try {
      dataService = await server.getService(config.imageProviderId);
      console.log(`Received image provider service with ID: ${config.imageProviderId}`);
    } catch (e) {
      console.error(e);
      await api.alert(
        `The image provider cannot be reached (ID: ${config.imageProviderId}). Please check if the service is running.`
      );
    }
  } else {
    const msg = "No annotation service ID provided in the configuration. Using example image.";
    console.log(msg);
    await api.alert(msg);
  }

  // Get the SAM service from the server
  try {
    samService = await server.getService(config.samServiceId, { mode: "first" });
    console.log(`Received SAM service with ID: ${config.samServiceId}`);
  } catch (e) {
    samService = null;
    console.error(e);
    await api.alert(
      `The SAM service is currently not reachable (ID: ${config.samServiceId}). Please wait a few minutes and reload the page to try again.`,
      { duration: 6000 }
    );
  }

  // Return the services
  return [ dataService, samService ];
};

const setImageLayer = async (viewer, dataService) => { 
    let image;
    let filename;
    if (dataService) {
      // Fetch a random image from the data provider if available
      console.log("Loading random image from the data provider...");
      [ image, filename ] = await dataService.get_random_image();
      
    } else {
      // Load an example image if no data provider is available
      console.log("Loading example image...");
      image = "https://raw.githubusercontent.com/bioimage-io/bioimageio-colab/main/data/example_image.png";
      filename = "example_image";
    }
    console.log("Image loaded:", filename);
    let imageLayer = await viewer.view_image(image, { name: filename });
    console.log("Image displayed:", filename);

    return imageLayer;
};

const getImageFromLayer = async (imageLayer) => {
  const vtkImage = await imageLayer.get_image();
  const pointData = await vtkImage.getPointData()
  const scalars = await pointData.getScalars();
  
  // Pixel data
  let pixelArray = await scalars.getData();
  const originalDtype = pixelArray.constructor;

  // Data type
  let dType = originalDtype.name;
  dType = dType.replace('Array', '').toLowerCase();
  
  // Dimensions
  const dimensions = await vtkImage.getDimensions();
  const width = dimensions[0];
  const height = dimensions[1];
  const nChannels = await scalars.getNumberOfComponents();

  if (nChannels === 4) {
    // Convert RGBA to RGB
    console.log('The image is in RGBA format. Converting to RGB...');
    const rgbArray = new originalDtype(width * height * 3);
    for (let i = 0; i < width * height; i++) {
      rgbArray[3 * i] = pixelArray[4 * i];  // R
      rgbArray[3 * i + 1] = pixelArray[4 * i + 1];  // G
      rgbArray[3 * i + 2] = pixelArray[4 * i + 2];  // B
    }
    pixelArray = rgbArray;
  }
  
  // Create the image object
  const image = {
    _rtype: "ndarray",
    _rvalue: pixelArray,
    _rshape: [width, height, 3],
    _rdtype: dType,
  };
  console.log('===== image =====>', image)
  return image;
};

const createTensorFromUint8Array = (data, shape) => {
  const dst = new ArrayBuffer(data.byteLength);
  new Uint8Array(dst).set(new Uint8Array(data));
  return new ort.Tensor("float32", new Float32Array(dst), shape);
};

const computeEmbedding = async (samService, image, modelID) => {
  // TODO: Load precomputed embedding for sample images

  // Compute the embedding for the image
  if (!samService) {
      const msg = "No SAM service available. Embedding computation was skipped.";
      console.log(msg);
      await api.showMessage(msg);
      return;
  }

  console.log(`Computing embedding for image with model ${modelID}...`);
  const embeddingPromise = samService.compute_embedding(image, modelID)
    .then(embeddingResult => {
      console.log("===== embeddingResult =====>", embeddingResult)

      // Set the model scale based on the image dimensions
      // TODO: check how to use embeddingResult.input_size (directly calculate modelScale)
      const longSideLength = Math.max(...embeddingResult["input_size"]);
      const w = image._rshape[0];
      const h = image._rshape[1];
      const samScale = longSideLength / Math.max(h, w);
      const modelScale = { height: h, width: w, samScale };
      console.log("===== modelScale =====>", modelScale)

      // Create the embedding tensor
      const embeddingTensor = createTensorFromUint8Array(
        embeddingResult.features._rvalue,
        embeddingResult.features._rshape
      )
      console.log("===== embeddingTensor =====>", embeddingTensor)

      return { embeddingTensor, modelScale };
    })
    .catch(error => {
      // Catch any errors during the embedding calculation or tensor preparation
      console.error('An error occurred while preparing the embedding:', error);
      throw error; // Propagate the error to be handled later
    });

  return embeddingPromise;
};

const prepareModelData = ({ embeddingResult, coordinates }) => {
  let pointCoords;
  let pointLabels;
  let pointCoordsTensor;
  let pointLabelsTensor;

  const embeddingTensor = embeddingResult.embeddingTensor;
  const modelScale = embeddingResult.modelScale;

  // Prepare input for the model
  const clicks = [ {
      x: coordinates[0], 
      y: coordinates[1], 
      clickType: 1
  } ];

  console.log('===== clicks =====>', clicks)

  // Check there are input click prompts
  if (clicks) {
    let n = clicks.length;

    // If there is no box input, a single padding point with 
    // label -1 and coordinates (0.0, 0.0) should be concatenated
    // so initialize the array to support (n + 1) points.
    pointCoords = new Float32Array(2 * (n + 1));
    pointLabels = new Float32Array(n + 1);

    // Add clicks and scale to what SAM expects
    for (let i = 0; i < n; i++) {
      pointCoords[2 * i] = clicks[i].x * modelScale.samScale;
      pointCoords[2 * i + 1] = clicks[i].y * modelScale.samScale;
      pointLabels[i] = clicks[i].clickType;
    }

    // Add in the extra point/label when only clicks and no box
    // The extra point is at (0, 0) with label -1
    pointCoords[2 * n] = 0.0;
    pointCoords[2 * n + 1] = 0.0;
    pointLabels[n] = -1.0;

    // Create the tensor
    pointCoordsTensor = new ort.Tensor("float32", pointCoords, [1, n + 1, 2]);
    pointLabelsTensor = new ort.Tensor("float32", pointLabels, [1, n + 1]);
  }
  const imageSizeTensor = new ort.Tensor("float32", [
    modelScale.height,
    modelScale.width,
  ]);

  if (pointCoordsTensor === undefined || pointLabelsTensor === undefined)
    return;

  // There is no previous mask, so default to an empty tensor
  const maskInput = new ort.Tensor(
    "float32",
    new Float32Array(256 * 256),
    [1, 1, 256, 256]
  );
  // There is no previous mask, so default to 0
  const hasMaskInput = new ort.Tensor("float32", [0]);

  const feeds = {
    image_embeddings: embeddingTensor,
    point_coords: pointCoordsTensor,
    point_labels: pointLabelsTensor,
    orig_im_size: imageSizeTensor,
    mask_input: maskInput,
    has_mask_input: hasMaskInput,
  };
  console.log('===== feeds =====>', feeds)
  return feeds;
};

const processMaskToGeoJSON = (masks) => {
  // Dimensions of the mask (batch, channels, width, height)
  const [b, c, width, height] = masks.dims;

  // 1. Apply threshold to create binary mask
  const binaryMask = new Uint8Array(width * height);
  for (let i = 0; i < masks.data.length; i++) {
    binaryMask[i] = masks.data[i] > 0.0 ? 255 : 0;
  }

  // Convert binaryMask to an OpenCV.js Mat
  const binaryMat = new cv.Mat(height, width, cv.CV_8UC1);
  binaryMat.data.set(binaryMask);

  // 2. Use OpenCV.js to find contours
  const contours = new cv.MatVector();
  const hierarchy = new cv.Mat();
  cv.findContours(
    binaryMat,
    contours,
    hierarchy,
    cv.RETR_EXTERNAL, // Retrieve only external contours
    cv.CHAIN_APPROX_SIMPLE // Compress horizontal, vertical, and diagonal segments
  );

  // 3. Process contours into GeoJSON-compatible features
  const features = [];

  if (contours.size() > 0) {
    // Pick the largest contour as the main object
    let largestContourIndex = 0;
    let largestContourSize = 0;
    for (let i = 0; i < contours.size(); i++) {
      const c = contours.get(i);
      if (c.rows > largestContourSize) {
        largestContourSize = c.rows;
        largestContourIndex = i;
      }
    }

    const largestContour = contours.get(largestContourIndex);
    const pts = [];

    for (let i = 0; i < largestContour.rows; i++) {
      const x = largestContour.intPtr(i)[0]; // x coordinate
      const y = largestContour.intPtr(i)[1]; // y coordinate
      pts.push([x, y]);
    }

    // Close the polygon if not closed
    if (
      pts.length > 0 &&
      (pts[0][0] !== pts[pts.length - 1][0] || pts[0][1] !== pts[pts.length - 1][1])
    ) {
      pts.push(pts[0]);
    }

    // Add the polygon to the features array
    features.push(pts);
  }

  console.log('===== contours =====>', features);

  // Clean up memory
  contours.delete();
  hierarchy.delete();
  binaryMat.delete();

  return features;
};

const addMaskToLayer = async (annotationLayer, masks, edgeColor) => {
  // Process the masks into GeoJSON-compatible features
  const features = processMaskToGeoJSON(masks);

  // Add the segmented features as polygons to the annotation layer
  for (let coords of features) {
      const polygon = {
          type: "Feature",
          coordinates: coords,
          geometry: {
              type: "Polygon",
              coordinates: [coords],
          },
          properties: {
              edge_color: edgeColor,
              edge_width: 2,
              size: 7,
          },
      };
      annotationLayer.add_feature(polygon);
  }

  return annotationLayer;
};

const setAnnotationLayer = async (viewer, edgeColor, samService, embeddingPromise, modelPromise) => {
  // Add the annotation functionality to the interface
  let annotationLayer
  annotationLayer = await viewer.add_shapes([], {
      shape_type: "polygon",
      draw_edge_color: edgeColor,
      name: "annotation",
      _rintf: true,
      // Callback for adding a new feature (annotation point)
      add_feature_callback: async (shape) => {
          if (shape.geometry.type === "Point") {
            if (!samService) {
              const msg = "No SAM service available. Embedding computation was skipped.";
              console.log(msg);
              await api.showMessage(msg);
              return;
            }
            // Segment the image and add the mask to the annotation layer
            const embeddingResult = await embeddingPromise;
            const feeds = prepareModelData({
                embeddingResult: embeddingResult,
                coordinates: shape.geometry.coordinates,
            });
            if (feeds === undefined) {
              console.log("No input data available for model.");
              return;
            }
            const model = await modelPromise; 
            const results = await model.run(feeds);
            console.log("===== results =====>", results);
            annotationLayer = await addMaskToLayer(annotationLayer, results["masks"], edgeColor);
          }
      }
  });
  return annotationLayer;
};

const saveAnnotation = async (dataService, imageLayer, annotationLayer) => {
    // Do not save if no data service or annotation layer is available
    
    if (!dataService) {
      const msg = "No data service provided. Saving was skipped.";
      console.log(msg);
      await api.showMessage(msg);
      return;
    }
    // Get the annotation features from the layer
    if (!annotationLayer) {
      const msg = "No annotation provided. Saving was skipped.";
      console.log(msg);
      await api.showMessage(msg);
      return;
    }
    const annotation = await annotationLayer.get_features();
    if (annotation.features.length > 0) {
        const image = await imageLayer.get_image();
        const dimensions = await image.getDimensions();
        await dataService.save_annotation(filename, annotation, [dimensions[0], dimensions[1]]);
        const msg = "Annotation saved.";
        console.log(msg);
        await api.showMessage(msg);
    } else {
      const msg = "No annotation provided. Saving was skipped.";
      console.log(msg);
      await api.showMessage(msg);
    }
};


// Define the BioImageIOColabAnnotator class
class BioImageIOColabAnnotator {
  constructor() {
    this.imageLayer = null; // Layer displaying the image
    this.annotationLayer = null; // Layer displaying the annotations
    this.edgeColor = "magenta"; // Default edge color for annotations
    this.modelID = "sam_vit_b_lm"; // Model name for the embedding
    this.modelPromise = null; // Promise for loading the model
  }

  async setup() {
    // No setup actions required for now
  }

  async run(ctx) {
    // Create and display the viewer window
    const viewer = await api.createWindow({src: "https://kaibu.org/#/app", fullscreen: true});

    // Get the services
    const [ dataService, samService ] = await getServices(ctx);

    const setModel = async () => {
      // Load the decoder for the selected model
      this.modelPromise = loadSamDecoder(this.modelID);
    };

    // Function to load an image and display it in the viewer with the annotation layer
    const setImageAnnotation = async () => {
      // First remove existing layers from the viewer
      await viewer.clear_layers();
      // Then load new image and annotation layers
      this.imageLayer = await setImageLayer(viewer, dataService);
      const image = await getImageFromLayer(imageLayer);
      const embeddingPromise = computeEmbedding(samService, image, this.modelID);
      this.annotationLayer = await setAnnotationLayer(viewer, this.edgeColor, samService, embeddingPromise, this.modelPromise);
    };
    
    // Function to load the next image
    const nextImage = async () => {
      // Save the current annotation if available
      await saveAnnotation(dataService, this.imageLayer, this.annotationLayer);
      // Load the next image
      await setImageAnnotation();
    };
    
    // Add a control widget with a button to load the next image
    await viewer.add_widget({
        _rintf: true,
        name: "Control",
        type: "control",
        elements: [
            {
                type: "button",
                label: "Save Annotation",
                callback: nextImage,
            }
        ],
    });

    // Start loading the model and the first image
    await setModel();
    await setImageAnnotation();
    await api.showMessage("Ready to annotate!");
  }
}

// Export the annotator class
api.export(new BioImageIOColabAnnotator());
</script>
