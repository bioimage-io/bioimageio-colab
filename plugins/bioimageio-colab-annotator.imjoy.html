<config lang="json">
{
  "name": "BioImage.IO Colab Annotator",
  "type": "iframe",
  "tags": [],
  "ui": "",
  "version": "0.1.0",
  "cover": "",
  "description": "Collaborative Annotator for BioImage.IO with Automated Segmentation",
  "icon": "extension",
  "inputs": null,
  "outputs": null,
  "api_version": "0.1.8",
  "env": "",
  "permissions": [],
  "requirements": [
    "https://cdn.jsdelivr.net/npm/hypha-rpc@0.20.38/dist/hypha-rpc-websocket.min.js",
    "https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js",
    "https://docs.opencv.org/4.5.0/opencv.js"
  ],
  "dependencies": []
}
</config>

<script lang="javascript">

async function processMaskToGeoJSON(input, width, height) {
  // 1. Create ImageData for visualization (white on transparent)
  //    Also create a binary mask for OpenCV.
  const arr = new Uint8ClampedArray(4 * width * height).fill(0);
  const binaryData = new Uint8Array(width * height);

  for (let i = 0; i < input.length; i++) {
    if (input[i] > 0.0) {
      // White pixel (255,255,255) with full alpha for the segmented area
      arr[4 * i + 0] = 255;
      arr[4 * i + 1] = 255;
      arr[4 * i + 2] = 255;
      arr[4 * i + 3] = 255;

      binaryData[i] = 255;
    } else {
      binaryData[i] = 0;
    }
  }

  const imageData = new ImageData(arr, width, height);

  // 2. Use OpenCV.js to find contours
  let src = cv.matFromArray(height, width, cv.CV_8UC1, binaryData);
  cv.threshold(src, src, 127, 255, cv.THRESH_BINARY);

  let contours = new cv.MatVector();
  let hierarchy = new cv.Mat();
  cv.findContours(src, contours, hierarchy, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE);

  let geoJSONFeature = null;
  let pts = [];
  // If at least one contour is found
  if (contours.size() > 0) {
    // Pick the largest contour as the main object
    let largestContourIndex = 0;
    let largestContourSize = 0;
    for (let i = 0; i < contours.size(); i++) {
      let c = contours.get(i);
      if (c.size() > largestContourSize) {
        largestContourSize = c.size();
        largestContourIndex = i;
      }
    }

    let contour = contours.get(largestContourIndex);
    
    for (let i = 0; i < contour.size(); i++) {
      let point = contour.get(i);
      // Add the point as [x, y]
      pts.push([point.x, point.y]);
    }

    // Close the polygon if not closed
    if (pts.length > 0 && (pts[0][0] !== pts[pts.length - 1][0] || pts[0][1] !== pts[pts.length - 1][1])) {
      pts.push(pts[0]);
    }

    
  }
    else {
        pts = []
    }
  

  // Cleanup OpenCV resources
  contours.delete();
  hierarchy.delete();
  src.delete();

  return pts;
}
    
const modelData = ({ clicks, tensor, modelScale }) => {
  const imageEmbedding = tensor;
  let pointCoords;
  let pointLabels;
  let pointCoordsTensor;
  let pointLabelsTensor;

  // Check there are input click prompts
  if (clicks) {
    let n = clicks.length;

    // If there is no box input, a single padding point with 
    // label -1 and coordinates (0.0, 0.0) should be concatenated
    // so initialize the array to support (n + 1) points.
    pointCoords = new Float32Array(2 * (n + 1));
    pointLabels = new Float32Array(n + 1);

    // Add clicks and scale to what SAM expects
    for (let i = 0; i < n; i++) {
      pointCoords[2 * i] = clicks[i].x * modelScale.samScale;
      pointCoords[2 * i + 1] = clicks[i].y * modelScale.samScale;
      pointLabels[i] = clicks[i].clickType;
    }

    // Add in the extra point/label when only clicks and no box
    // The extra point is at (0, 0) with label -1
    pointCoords[2 * n] = 0.0;
    pointCoords[2 * n + 1] = 0.0;
    pointLabels[n] = -1.0;

    // Create the tensor
    pointCoordsTensor = new ort.Tensor("float32", pointCoords, [1, n + 1, 2]);
    pointLabelsTensor = new ort.Tensor("float32", pointLabels, [1, n + 1]);
  }
  const imageSizeTensor = new ort.Tensor("float32", [
    modelScale.height,
    modelScale.width,
  ]);

  if (pointCoordsTensor === undefined || pointLabelsTensor === undefined)
    return;

  // There is no previous mask, so default to an empty tensor
  const maskInput = new ort.Tensor(
    "float32",
    new Float32Array(256 * 256),
    [1, 1, 256, 256]
  );
  // There is no previous mask, so default to 0
  const hasMaskInput = new ort.Tensor("float32", [0]);

  return {
    image_embeddings: imageEmbedding,
    point_coords: pointCoordsTensor,
    point_labels: pointLabelsTensor,
    orig_im_size: imageSizeTensor,
    mask_input: maskInput,
    has_mask_input: hasMaskInput,
  };
};
    
    
class BioImageIOColabAnnotator {
    constructor() {
        this.image = null; // Current image
        this.mask = null; // Current mask
        this.filename = null; // Filename of the current image
        this.imageLayer = null; // Layer displaying the image
        this.annotationLayer = null; // Layer displaying the annotations
        this.edgeColor = "magenta"; // Default edge color for annotations
        this.modelName = "vit_b_lm"; // Model name for the embeddings
    }

    async setup() {
        // No setup actions required for now
    }

    async run(ctx) {
        const model = await ort.InferenceSession.create("https://uk1s3.embassy.ebi.ac.uk/public-datasets/sam-vit_b_lm-decoder/1/model.onnx");
        console.log('onnx model created', model)
        // Extract configuration settings
        const config = ctx.config || {};
        const serverUrl = config.server_url || "https://hypha.aicell.io";
        const workspace = config.workspace;
        const token = config.token || await hyphaWebsocketClient.login({server_url: serverUrl})
        const annotationServiceId = config.annotation_service_id;
        if(!annotationServiceId){
            await api.alert("Please provide the annotation service ID in the configuration.");
            return;
        }
        const samServiceId = "bioimageio-colab/microsam";
        
        
        // Create and display the viewer window
        const viewer = await api.createWindow({src: "https://kaibu.org/#/app", fullscreen: true});
        await api.showMessage(`Connecting to server ${serverUrl}...`);
        // Login before connecting and then use userid
        // TODO: Add login functionality

        // Connect to the Hypha server
        const server = await hyphaWebsocketClient.connectToServer({
            server_url: serverUrl,
            token: token,
            workspace: workspace,
        });

        // Get the user ID
        const userID = server.config.user.id;
        await api.showMessage(`Connected to workspace ${server.config.user.scope.current_workspace} as user ${userID}.`);

        // Get the bioimageio-colab service from the server
        let dataProvider;
        try {
            dataProvider = await server.getService(annotationServiceId);
        } catch (e) {
            console.error(e)
            await api.alert(`Failed to get the bioimageio-colab annotation service (id=${annotationServiceId}). (Error: ${e})`);
            return;
        }

        // Get the SAM service from the server
        let samService;
        try {
            samService = await server.getService(samServiceId, {"mode": "random"});
        } catch (e) {
            samService = null;
            console.error(e)
            await api.showMessage(`Failed to get the bioimageio-colab SAM service (id=${samServiceId}). Please try again later.`);
        }

        // Flag to check if the image embedding is already calculated
        let embeddingIsCalculated = false; 

        // Function to get a new image and set up the viewer
        const getImage = async () => {
            if (this.image !== null) {
                // Remove existing layers if there is any image loaded
                await viewer.remove_layer({id: this.imageLayer.id});
                await viewer.remove_layer({id: this.annotationLayer.id});
            }

            // Fetch a random image from the service
            [this.image, this.filename] = await dataProvider.get_random_image();
            this.imageLayer = await viewer.view_image(this.image, {name: "image"});

            // Clear any previous image embeddings from the SAM service
            if (samService) {
                embeddingIsCalculated = false;
                await samService.clear_cache();
            }
            
            let embedding;

            // Add the annotation functionality to the interface
            this.annotationLayer = await viewer.add_shapes([], {
                shape_type: "polygon",
                draw_edge_color: this.edgeColor,
                name: "annotation",
                _rintf: true,
                // Callback for adding a new feature (annotation point)
                add_feature_callback: async (shape) => {
                    if (shape.geometry.type === "Point") {
                        if (samService) {
                            // The point coordinates need to be reversed to match the coordinate convention of SAM
                            const pointCoords = [shape.geometry.coordinates.reverse()];
                            const pointLabels = pointCoords.map(() => 1); // All points have a label of 1
                            let features = [];

                            // Compute embeddings if not already computed for the image
                            try {
                                if (!embeddingIsCalculated) {
                                    await api.showMessage("Computing embedding and segmenting image...");
                                    embedding = await samService.compute_embedding(this.modelName, this.image)
                                    console.log("===========>", embedding)
                                    embeddingIsCalculated = true;
                                } else {
                                    await api.showMessage("Segmenting...");
                                }

                                const dst = new ArrayBuffer(embedding.features._rvalue.byteLength);
                                new Uint8Array(dst).set(new Uint8Array(embedding.features._rvalue));
                                const b = new Float32Array(dst);
                                const featureTensor = new ort.Tensor(b, embedding.features._rshape)
                                console.log('=======input tensor===========>', featureTensor)
                                const feeds = modelData({
                                  clicks: pointCoords.map((coord)=> {
                                    return { x:coord[0], y:coord[1], clickType: 1 }
                                  }),
                                  tensor: featureTensor,
                                  modelScale: {
                                      height: embedding.input_size[1],  // original image height
                                      width: embedding.input_size[0],  // original image width
                                      samScale: 4, // scaling factor for image which has been resized to longest side 1024
                                    },
                                });
                                // feed inputs and run
                                const results = await model.run(feeds);
                                console.log("==========> results", results)
                                const output = results[model.outputNames[0]]
                                features = await processMaskToGeoJSON(output, embedding.input_size[0], embedding.input_size[1])
                                console.log('========features', features)
                                // features = await samService.segment(this.modelName, this.image, pointCoords, pointLabels);
                                
                                
                            } catch (e) {
                                console.error(e)
                                await api.showMessage(`Failed to compute the image embedding. (Error: ${e})`);
                                return;
                            }

                            // Add the segmented features as polygons to the annotation layer
                            for (let coords of features) {
                                const polygon = {
                                    type: "Feature",
                                    coordinates: coords,
                                    geometry: {
                                        type: "Polygon",
                                        coordinates: [coords],
                                    },
                                    properties: {
                                        edge_color: this.edgeColor,
                                        edge_width: 2,
                                        size: 7,
                                    },
                                };
                                this.annotationLayer.add_feature(polygon);
                            }
                        } else {
                            await api.showMessage("Warning: SAM service not available. Please try again later.");
                        }
                    }
                }
            });
        };

        // Function to save the annotation
        const saveAnnotation = async () => {
            if (!this.annotationLayer) return;
            const annotation = await this.annotationLayer.get_features();
            if (annotation.features.length > 0) {
                await dataProvider.save_annotation(this.filename, annotation, [this.image._rshape[0], this.image._rshape[1]]);
                await api.showMessage("Annotation saved.");
            } else {
                await api.showMessage("No annotation provided. Saving was skipped.");
            }
        };

        // Function to load the next image
        const nextImage = async () => {
            await saveAnnotation();
            await getImage();
        };

        // Add a control widget with a button to load the next image
        await viewer.add_widget({
            _rintf: true,
            name: "Control",
            type: "control",
            elements: [
                {
                    type: "button",
                    label: "Save Annotation",
                    callback: nextImage,
                }
            ],
        });

        // Load the initial image
        await getImage();
        await api.showMessage("Ready to annotate!");
    }
}

// Export the annotator class
api.export(new BioImageIOColabAnnotator());
</script>

