<config lang="json">
{
  "name": "BioImage.IO Colab Annotator",
  "type": "iframe",
  "tags": [],
  "ui": "",
  "version": "0.1.0",
  "cover": "",
  "description": "Collaborative Annotator for BioImage.IO with Automated Segmentation",
  "icon": "extension",
  "inputs": null,
  "outputs": null,
  "api_version": "0.1.8",
  "env": "",
  "permissions": [],
  "requirements": [
    "https://cdn.jsdelivr.net/npm/hypha-rpc@0.20.38/dist/hypha-rpc-websocket.min.js",
    "https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js",
    "https://docs.opencv.org/4.5.0/opencv.js"
  ],
  "dependencies": []
}
</config>

<script lang="javascript">
const modelUrls = {
  "vit_b_lm": "https://raw.githubusercontent.com/constantinpape/mbexc-review/refs/heads/master/vit_b_lm_decoder.onnx",
  "vit_b_em": "",
};
    
const loadSamDecoder = (modelName) => {
  "use strict";
  // Check if model in localStorage
  const modelUrl = modelUrls[modelName];
  console.log('Starting to created ONNX model from', modelUrl);
  const modelPromise = ort.InferenceSession.create(modelUrl)
    .then((model) => {
      console.log('ONNX model created', model);
      return model;
    })
    .catch((error) => {
      console.error('Error creating ONNX model:', error);
      throw error; // Propagate the error
    });

  return modelPromise;
};
    
const extractConfig = (ctx) => {
  "use strict";
  // Extract configuration settings and check if they are valid
  const config = ctx.config || {};

  if (!config.server_url) {
    console.log("No server URL provided in the configuration. Using the default server URL.");
    config.server_url = "https://hypha.aicell.io";
    config.workspace = null;
    config.token = null;
  } else {
    if (!config.workspace) {
      console.log("No workspace provided in the configuration.");
    }
    if (!config.token) {
      console.log("No token provided in the configuration.");
      // Login before connecting and then use userid
      // TODO: Add login functionality
    }
  }

  // if (!config.annotation_service_id) {
  //   console.log("No annotation service ID provided in the configuration. Using example image.");
  //   api.alert("No annotation service ID provided in the configuration. Using example image.");
  // }

  return {
    serverUrl: config.server_url,
    workspace: config.workspace,
    token: config.token,
    imageProviderId: config.annotation_service_id,
    samServiceId: "bioimageio-colab/microsam"
  };
};

    
const loadExampleImage = async (imageID) => {
  "use strict";
  try {
    const imageUrls = [
        'https://raw.githubusercontent.com/constantinpape/mbexc-review/refs/heads/master/cells.png',
        'https://raw.githubusercontent.com/constantinpape/mbexc-review/refs/heads/master/nuclei.png',
    ];
    console.log('Downloading image ID:', imageID);
    console.log('Downloading image from:', imageUrls[imageID]);

    const response = await fetch(imageUrls[imageID]);

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const blob = await response.blob();

    // Convert the blob to an ImageBitmap
    const imageBitmap = await createImageBitmap(blob);

    // Create a canvas to draw the image for pixel data extraction
    const canvas = document.createElement('canvas');
    const ctx = canvas.getContext('2d');

    // Set canvas dimensions to match the image
    canvas.width = imageBitmap.width;
    canvas.height = imageBitmap.height;

    // Draw the image onto the canvas
    ctx.drawImage(imageBitmap, 0, 0);

    // Get pixel data from the canvas
    const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
    const pixels = imageData.data; // This is a Uint8ClampedArray

    // Convert the pixel data into an ndarray-like format
    const image = {
      _rtype: "ndarray",
      _rvalue: pixels.buffer, // Use ArrayBuffer for the pixel data
      _rshape: [canvas.height, canvas.width, 4], // RGBA format (4 channels)
      _rdtype: "uint8" // Data type for the pixel values
    };
    const fileName = 'example_image.tif';

    console.log('Example image loaded into ndarray format');
    return [ image, fileName ];
  } catch (error) {
    console.error('Failed to load example image:', error);
    throw error;
  }
};
    
const downloadExampleEmbedding = async (imageID) => {
  "use strict";

  const embeddingUrls = [
      "https://raw.githubusercontent.com/constantinpape/mbexc-review/refs/heads/master/embeddings_cells.bin",
      "https://raw.githubusercontent.com/constantinpape/mbexc-review/refs/heads/master/embeddings_nuclei.bin",
  ];
  console.log('Downloading binary embedding from:', embeddingUrls[imageID]);
  
  try {
    const response = await fetch(embeddingUrls[imageID]);
    if (!response.ok) {
      throw new Error('Failed to fetch the embedding.');
    }
    const binaryEmbedding = await response.arrayBuffer();
    console.log('Embedding successfully downloaded.');
    return binaryEmbedding;
  } catch (error) {
    console.error('Error downloading embedding:', error);
    throw error; // Rethrow error to propagate it
  }
};
    
const loadBinaryTensor = async (binaryEmbedding, embeddingShape) => {
  "use strict";
  try {
    const dst = new ArrayBuffer(binaryEmbedding.byteLength);
    new Uint8Array(dst).set(new Uint8Array(binaryEmbedding));
    const b = new Float32Array(dst);
    const dType = "float32";
    const featureTensor = new ort.Tensor(dType, b, embeddingShape);
    console.log('Tensor successfully created.');
    return featureTensor;
  } catch (error) {
    console.error('Error creating tensor from binary embedding:', error);
    throw error; // Rethrow error to propagate it
  }
};
    
const loadExampleEmbedding = async (imageID) => {
  "use strict";
  const embeddingShape = [1, 256, 64, 64];
  console.log('Starting to load example image embedding...');

  // Return the promise directly
  const embeddingPromise = downloadExampleEmbedding(imageID)
    .then(binaryEmbedding => loadBinaryTensor(binaryEmbedding, embeddingShape))
    .then(featureTensor => {
      console.log('Example embedding has been successfully downloaded and prepared.');
      return featureTensor; // Return the tensor for use later
    })
    .catch(error => {
      console.error('An error occurred while preparing the embedding:', error);
      throw error; // Propagate the error
    });
  return embeddingPromise;
};

const setModelScale = (image) => {
  "use strict";
  console.log("Calculating model scale");
  const LONG_SIDE_LENGTH = 1024;
  const w = image._rshape[0];
  const h = image._rshape[1];
  const samScale = LONG_SIDE_LENGTH / Math.max(h, w);
  const modelScale = { height: h, width: w, samScale };
  console.log("Set model scale to:", modelScale);
  return modelScale;
};
    
const setModelData = ({ clicks, embeddings, modelScale }) => {
  let pointCoords;
  let pointLabels;
  let pointCoordsTensor;
  let pointLabelsTensor;

  // Check there are input click prompts
  if (clicks) {
    let n = clicks.length;

    // If there is no box input, a single padding point with 
    // label -1 and coordinates (0.0, 0.0) should be concatenated
    // so initialize the array to support (n + 1) points.
    pointCoords = new Float32Array(2 * (n + 1));
    pointLabels = new Float32Array(n + 1);

    // Add clicks and scale to what SAM expects
    for (let i = 0; i < n; i++) {
      pointCoords[2 * i] = clicks[i].x * modelScale.samScale;
      pointCoords[2 * i + 1] = clicks[i].y * modelScale.samScale;
      pointLabels[i] = clicks[i].clickType;
    }

    // Add in the extra point/label when only clicks and no box
    // The extra point is at (0, 0) with label -1
    pointCoords[2 * n] = 0.0;
    pointCoords[2 * n + 1] = 0.0;
    pointLabels[n] = -1.0;

    // Create the tensor
    pointCoordsTensor = new ort.Tensor("float32", pointCoords, [1, n + 1, 2]);
    pointLabelsTensor = new ort.Tensor("float32", pointLabels, [1, n + 1]);
  }
  const imageSizeTensor = new ort.Tensor("float32", [
    modelScale.height,
    modelScale.width,
  ]);

  if (pointCoordsTensor === undefined || pointLabelsTensor === undefined) {
    return;
  }

  // There is no previous mask, so default to an empty tensor
  const maskInput = new ort.Tensor(
    "float32",
    new Float32Array(256 * 256),
    [1, 1, 256, 256]
  );
  // There is no previous mask, so default to 0
  const hasMaskInput = new ort.Tensor("float32", [0]);

  return {
    image_embeddings: embeddings,
    point_coords: pointCoordsTensor,
    point_labels: pointLabelsTensor,
    orig_im_size: imageSizeTensor,
    mask_input: maskInput,
    has_mask_input: hasMaskInput,
  };
};
    
const processMaskToGeoJSON = (masks) => {
  // Dimensions of the mask (batch, channels, width, height)
  const [b, c, width, height] = masks.dims;

  // 1. Apply threshold to create binary mask
  const binaryMask = new Uint8Array(width * height);
  for (let i = 0; i < masks.data.length; i++) {
    binaryMask[i] = masks.data[i] > 0.0 ? 255 : 0;
  }

  // Convert binaryMask to an OpenCV.js Mat
  const binaryMat = new cv.Mat(height, width, cv.CV_8UC1);
  binaryMat.data.set(binaryMask);

  // 2. Use OpenCV.js to find contours
  const contours = new cv.MatVector();
  const hierarchy = new cv.Mat();
  cv.findContours(
    binaryMat,
    contours,
    hierarchy,
    cv.RETR_EXTERNAL, // Retrieve only external contours
    cv.CHAIN_APPROX_SIMPLE // Compress horizontal, vertical, and diagonal segments
  );

  // 3. Process contours into GeoJSON-compatible features
  const features = [];

  if (contours.size() > 0) {
    // Pick the largest contour as the main object
    let largestContourIndex = 0;
    let largestContourSize = 0;
    for (let i = 0; i < contours.size(); i++) {
      const c = contours.get(i);
      if (c.rows > largestContourSize) {
        largestContourSize = c.rows;
        largestContourIndex = i;
      }
    }

    const largestContour = contours.get(largestContourIndex);
    const pts = [];

    for (let i = 0; i < largestContour.rows; i++) {
      const x = largestContour.intPtr(i)[0]; // x coordinate
      const y = largestContour.intPtr(i)[1]; // y coordinate
      pts.push([x, y]);
    }

    // Close the polygon if not closed
    if (
      pts.length > 0 &&
      (pts[0][0] !== pts[pts.length - 1][0] || pts[0][1] !== pts[pts.length - 1][1])
    ) {
      pts.push(pts[0]);
    }

    // Add the polygon to the features array
    features.push(pts);
  }

  console.log('===== contours =====>', features);

  // Clean up memory
  contours.delete();
  hierarchy.delete();
  binaryMat.delete();

  return features;
};

    
class BioImageIOColabAnnotator {
    constructor() {
        this.image = null; // Current image
        this.filename = null; // Filename of the current image
        this.mask = null; // Current mask
        this.imageLayer = null; // Layer displaying the image
        this.annotationLayer = null; // Layer displaying the annotations
        this.edgeColor = "magenta"; // Default edge color for annotations
        this.modelName = "vit_b_lm"; // Model name for the embeddings
        this.imageID = 0; // The id of the image to load.
    }

    async setup() {
        // No setup actions required for now
    }

    async run(ctx) {
        // Create and display the viewer window
        const viewer = await api.createWindow({src: "https://kaibu.org/#/app", fullscreen: true});
        
        // Check the configuration
        const config = await extractConfig(ctx);
        
        // Start loading the model
        const modelPromise = loadSamDecoder(this.modelName);
        
        let embeddingsPromise;
        let modelScale;
        let model;

        // Function to get a new image and set up the viewer
        const getImage = async () => {
            if (this.image !== null) {
                // Remove existing layers if there is any image loaded
                await viewer.remove_layer({id: this.imageLayer.id});
                await viewer.remove_layer({id: this.annotationLayer.id});
            }
            
            // Reset previous embedding
            embeddingsPromise = null;

            [this.image, this.filename] = await loadExampleImage(this.imageID);

            // Create a promise for the example embedding calculation, to be awaited later
            embeddingsPromise = loadExampleEmbedding(this.imageID);
            
            // Display the image immediately
            this.imageLayer = await viewer.view_image(this.image, { name: "image" });
            
            // Set the model scale
            modelScale = setModelScale(this.image);
            
            // Add the annotation functionality to the interface
            this.annotationLayer = await viewer.add_shapes([], {
                shape_type: "polygon",
                draw_edge_color: this.edgeColor,
                name: "annotation",
                _rintf: true,
                // Callback for adding a new feature (annotation point)
                add_feature_callback: async (shape) => {
                    if (shape.geometry.type === "Point") {
                        if (!embeddingsPromise) {
                          await api.showMessage("Automated segmentation is disabled while SAM service can't be reached. Reload the page to try again.");  
                          return;
                        }
                        
                        // Prepare input for the model
                        const clicks = [ {
                            x: shape.geometry.coordinates[0], 
                            y: shape.geometry.coordinates[1], 
                            clickType: 1
                        } ];
                        // await api.showMessage("Computing image embeddings...");
                        const embeddings = await embeddingsPromise;
                        const feeds = setModelData({
                          clicks: clicks,
                          embeddings: embeddings,
                          modelScale: modelScale
                        });
                        console.log('===== feeds =====>', feeds);
                        if (feeds === undefined) return;
                        
                        // Feed inputs and run
                        model = await modelPromise;
                        const results = await model.run(feeds);
                        console.log("===== results =====>", results);
                        
                        // Convert the mask to GeoJSON
                        const masks = results["masks"];
                        const features = processMaskToGeoJSON(masks);

                        // Add the segmented features as polygons to the annotation layer
                        for (let coords of features) {
                            const polygon = {
                                type: "Feature",
                                coordinates: coords,
                                geometry: {
                                    type: "Polygon",
                                    coordinates: [coords],
                                },
                                properties: {
                                    edge_color: this.edgeColor,
                                    edge_width: 2,
                                    size: 7,
                                },
                            };
                            this.annotationLayer.add_feature(polygon);
                        }
                    }
                }
            });
            
            this.imageId = this.ImageID + 1;
            // TODO wrap around if we are at the max image id
        };
        
        // Function to load the next image
        const nextImage = async () => {
            await getImage();
            this.imageID++;
        };
        
        // Add a control widget with a button to load the next image
        await viewer.add_widget({
            _rintf: true,
            name: "Control",
            type: "control",
            elements: [
                {
                    type: "button",
                    label: "Next Image",
                    callback: nextImage,
                }
            ],
        });

        // Load the initial image
        await getImage();
        await api.showMessage("Ready to annotate!");
    }
}

// Export the annotator class
api.export(new BioImageIOColabAnnotator());
</script>
