{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwEzvpbdOYKT"
      },
      "source": [
        "# BioImageIO Colab - CellPose Finetuning\n",
        "\n",
        "This notebook demonstrates how to use image annotations collected from [BioImage.IO Colab](https://bioimage-io.github.io/bioimageio-colab/) to train and fine-tune a CellPose model.\n",
        "\n",
        "For more details on cellpose 2.0 check out the [paper](https://www.biorxiv.org/content/10.1101/2022.04.01.486764v1) or the [talk](https://www.youtube.com/watch?v=3ydtAhfq6H0).\n",
        "\n",
        "*Most of this notebook is based on the original [CellPose notebook](https://github.com/mouseland/cellpose).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbqFni8kuFar"
      },
      "source": [
        "## 0.1. Installation\n",
        "\n",
        "We will first install all the dependencies required for cellpose 2.0. By default the torch GPU version is installed in COLAB notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jlMnqge-lQ9s"
      },
      "outputs": [],
      "source": [
        "#@markdown ###Install these required dependencies:\n",
        "\n",
        "#@markdown * tifffile\n",
        "#@markdown * matplotlib\n",
        "#@markdown * opencv-python-headless\n",
        "#@markdown * cellpose\n",
        "\n",
        "## Install required dependencies\n",
        "\n",
        "!pip install tifffile matplotlib \"opencv-python-headless<=4.3\" cellpose\n",
        "\n",
        "#@markdown You will have to restart the runtime after this finishes to include the new packages. In the menu above do: Runtime --> Restart session\n",
        "\n",
        "#@markdown Don't worry about all the errors that pip give below, these are resolved in the end. We apologise for the ugly installation - a consequence of using Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heEiTSWQZZ6y"
      },
      "source": [
        "## 0.2. Mount google drive\n",
        "\n",
        "Please mount your google drive and find your [BioImage.IO Colab](https://bioimage-io.github.io/bioimageio-colab/) folder with source images and annotations. This also ensures that any models you train are saved to your google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uGUNrjdRfVDs"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@markdown ###Connect your Google Drive to Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "#@markdown * Connect to Google Drive.\n",
        "\n",
        "#@markdown * Sign into your Google Account.\n",
        "\n",
        "#@markdown * Click \"Continue\"\n",
        "\n",
        "#@markdown * Select the permission to \"See, edit, create, and delete all of your Google Drive files\".\n",
        "\n",
        "#@markdown * Click \"Continue\"\n",
        "\n",
        "#@markdown Your Google Drive folder should now be available here as \"gdrive\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAGzqj_kQhzr"
      },
      "source": [
        "# 1. Display manual annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "odfAaWFKurc-"
      },
      "outputs": [],
      "source": [
        "#@markdown ###Select your mounted folder from BioImage.IO Colab:\n",
        "import os\n",
        "\n",
        "path2images = \"/content/gdrive/MyDrive/hpa_demo\" #@param {type:\"string\"}\n",
        "path2annotations = os.path.join(path2images, \"annotations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpiJTsngv6TJ"
      },
      "source": [
        "We will first match all pairs of source images and annotation masks and then display up to 6 of these pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt0Y5zVJ4YuI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tifffile import imread\n",
        "\n",
        "\n",
        "# List to hold pairs of image and corresponding annotation masks\n",
        "image_annotation_pairs = []\n",
        "\n",
        "# Get list of all images and annotations\n",
        "image_files = os.listdir(path2images)\n",
        "annotation_files = os.listdir(path2annotations)\n",
        "\n",
        "# Iterate through each image in the images folder\n",
        "for image_file in image_files:\n",
        "    # Get the base name of the image file (without extension)\n",
        "    image_name = os.path.splitext(image_file)[0]\n",
        "\n",
        "    # Find all corresponding annotation masks in the annotations folder\n",
        "    corresponding_masks = [os.path.join(path2annotations, annotation_file)\n",
        "                           for annotation_file in annotation_files\n",
        "                           if annotation_file.startswith(image_name) and annotation_file.endswith('.tif')]\n",
        "\n",
        "    # If any corresponding masks are found, add them as tuples to the list\n",
        "    if corresponding_masks:\n",
        "        image_path = os.path.join(path2images, image_file)\n",
        "        for mask in corresponding_masks:\n",
        "            image_annotation_pairs.append((image_path, mask))\n",
        "\n",
        "# Print the numer of annotations\n",
        "num_pairs = len(image_annotation_pairs)\n",
        "print(f\"Number of annotations: {num_pairs}\")\n",
        "\n",
        "def read_image(path):\n",
        "    img = imread(path)\n",
        "    if img.ndim == 3 and img.shape[0] == 3:\n",
        "        img = np.transpose(img, [1, 2, 0])\n",
        "    return img\n",
        "\n",
        "if num_pairs < 6:\n",
        "    # Plot one single annotation starting from the first pair\n",
        "    k = 0\n",
        "    plt.figure(figsize=(10, 20))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(read_image(image_annotation_pairs[k][0]))\n",
        "    plt.title(f\"Image: {os.path.basename(image_annotation_pairs[k][0])}\")\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(read_image(image_annotation_pairs[k][1]))\n",
        "    plt.title(f\"Annotation: {os.path.basename(image_annotation_pairs[k][1])}\")\n",
        "    plt.show()\n",
        "else:\n",
        "    # Plot several random annotations\n",
        "    choices = np.random.choice(num_pairs, 6, replace=False)\n",
        "    plt.figure(figsize=(17, 5))\n",
        "    for i in range(6):\n",
        "        plt.subplot(2, 6, 2 * (i + 1) - 1)\n",
        "        plt.imshow(read_image(image_annotation_pairs[choices[i]][0]))\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"{os.path.basename(image_annotation_pairs[choices[i]][0])}\")\n",
        "        plt.subplot(2, 6, 2 * (i + 1))\n",
        "        plt.imshow(read_image(image_annotation_pairs[choices[i]][1]))\n",
        "        plt.axis('off')\n",
        "        plt.title(\"Annotation\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7c7V4yEqDc_"
      },
      "source": [
        "# 2. Running cellpose 2.0 with a GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Tt8hgC7rniP8"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Check CUDA version and that GPU is working in cellpose. Also import other libraries.\n",
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "\n",
        "import shutil\n",
        "from cellpose import core, utils, io, models, metrics\n",
        "from glob import glob\n",
        "\n",
        "use_GPU = core.use_gpu()\n",
        "yn = ['NO', 'YES']\n",
        "print(f'>>> GPU activated? {yn[use_GPU]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfE75htF0l84"
      },
      "source": [
        "##  2.1. Split manual annotations into train and test\n",
        "\n",
        "**Paths for training, predictions and results**\n",
        "\n",
        "**`train_dir:`, `test_dir`:** These are the paths to your folders train_dir (with images and masks of training images) and test_dir (with images and masks of test images). You can leave the test_dir blank, but it's recommended to have some test images to check the model's performance. To find the paths of the folders containing the respective datasets, go to your Files on the left of the notebook, navigate to the folder containing your files and copy the path by right-clicking on the folder, **Copy path** and pasting it into the right box below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ixDeOX-bHeux"
      },
      "outputs": [],
      "source": [
        "from tifffile import imwrite\n",
        "\n",
        "#@markdown Run this cell to split your pairs of images and annotations into training and testing groups and then save them to `train_dir`and `test_dir`.\n",
        "\n",
        "#@markdown ###Path to images and masks:\n",
        "train_dir = \"/content/train\" #@param {type:\"string\"}\n",
        "test_dir = \"/content/test\" #@param {type:\"string\"}\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Check if the folders are empty\n",
        "if os.listdir(train_dir):\n",
        "    print(f\"Warning: Folder '{train_dir}' is not empty.\")\n",
        "if os.listdir(test_dir):\n",
        "    print(f\"Warning: Folder '{test_dir}' is not empty.\")\n",
        "\n",
        "# Define where the patch file will be saved\n",
        "base = os.path.dirname(train_dir)\n",
        "\n",
        "#@markdown ###Define the test portion\n",
        "\n",
        "test_portion = 0.2 #@param {type:\"slider\", min:0.01, max:0.99, step:0.01}\n",
        "\n",
        "# Calculate the number of test images\n",
        "t = np.floor(test_portion * len(image_annotation_pairs)).astype(int)\n",
        "\n",
        "# Function to convert RGB images to grayscale\n",
        "def rgb2gray(image):\n",
        "    if len(image.shape) == 3:\n",
        "        if image.shape[0] == 3:\n",
        "            image = np.transpose(image, [1, 2, 0])\n",
        "        image = np.float32(image)\n",
        "        image = np.dot(image[..., :3], [0.2989, 0.5870, 0.1140])\n",
        "        image = np.int8(image)\n",
        "    return image\n",
        "\n",
        "# Helper function to generate new filenames\n",
        "def generate_new_filenames(image_path, annotation_path):\n",
        "    image_name = os.path.basename(image_path)\n",
        "    base_name = image_name.split('.tif')[0]\n",
        "\n",
        "    # Extract the mask number from the annotation filename\n",
        "    mask_number = annotation_path.split('_mask_')[-1].split('.tif')[0]\n",
        "\n",
        "    # Generate new filenames\n",
        "    new_image_name = f\"{base_name}_{mask_number}.tif\"\n",
        "    new_annotation_name = f\"{base_name}_{mask_number}_seg.tif\"\n",
        "\n",
        "    return new_image_name, new_annotation_name\n",
        "\n",
        "# Split the data into training and test sets\n",
        "for i in range(len(image_annotation_pairs) - t):\n",
        "    image_path, annotation_path = image_annotation_pairs[i]\n",
        "\n",
        "    # Generate new filenames\n",
        "    new_image_name, new_annotation_name = generate_new_filenames(image_path, annotation_path)\n",
        "\n",
        "    # Process and save the training images\n",
        "    image = imread(image_path)\n",
        "    image = rgb2gray(image)\n",
        "    imwrite(os.path.join(train_dir, new_image_name), image)\n",
        "\n",
        "    # Copy the corresponding annotation with the new name\n",
        "    shutil.copyfile(annotation_path, os.path.join(train_dir, new_annotation_name))\n",
        "\n",
        "for i in range(len(image_annotation_pairs) - t, len(image_annotation_pairs)):\n",
        "    image_path, annotation_path = image_annotation_pairs[i]\n",
        "\n",
        "    # Generate new filenames\n",
        "    new_image_name, new_annotation_name = generate_new_filenames(image_path, annotation_path)\n",
        "\n",
        "    # Process and save the test images\n",
        "    image = imread(image_path)\n",
        "    image = rgb2gray(image)\n",
        "    imwrite(os.path.join(test_dir, new_image_name), image)\n",
        "\n",
        "    # Copy the corresponding annotation with the new name\n",
        "    shutil.copyfile(annotation_path, os.path.join(test_dir, new_annotation_name))\n",
        "\n",
        "# Print the number of training and test images\n",
        "print(f\"Training images: {len(os.listdir(train_dir)) // 2}\")\n",
        "print(f\"Test images: {len(os.listdir(test_dir)) // 2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLdKNWQ4jxy5"
      },
      "source": [
        "## 2.2. Training parameters\n",
        "\n",
        "**Pretrained model and new model name**\n",
        "\n",
        "Fill out the form below with the parameters to start training.\n",
        "\n",
        "**`initial_model`:** Choose a model from the cellpose [model zoo](https://cellpose.readthedocs.io/en/latest/models.html#model-zoo) to start from.\n",
        "\n",
        "**`model_name`**: Enter the path where your model will be saved once trained (for instance your result folder).\n",
        "\n",
        "**Training parameters**\n",
        "\n",
        "**`number_of_epochs`:** Input how many epochs (rounds) the network will be trained. At least 100 epochs are recommended, but sometimes 250 epochs are necessary, particularly from scratch. **Default value: 100**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XQI4aUxCjz3n"
      },
      "outputs": [],
      "source": [
        "# model name and path\n",
        "#@markdown ###Name of the pretrained model to start from and new model name:\n",
        "from cellpose import models\n",
        "initial_model = \"cyto3\" #@param [\"cyto\", \"cyto3\",\"nuclei\",\"tissuenet_cp3\", \"livecell_cp3\", \"yeast_PhC_cp3\", \"yeast_BF_cp3\", \"bact_phase_cp3\", \"bact_fluor_cp3\", \"deepbacs_cp3\", \"scratch\"]\n",
        "model_name = \"CP_HPA_CrowdSourcing\" #@param {type:\"string\"}\n",
        "\n",
        "# other parameters for training.\n",
        "#@markdown ###Training Parameters:\n",
        "#@markdown Number of epochs:\n",
        "n_epochs =  10#@param {type:\"number\"}\n",
        "\n",
        "Channel_to_use_for_training = \"Grayscale\" #@param [\"Grayscale\", \"Blue\", \"Green\", \"Red\"]\n",
        "\n",
        "# @markdown ###If you have a secondary channel that can be used for training, for instance nuclei, choose it here:\n",
        "\n",
        "Second_training_channel= \"None\" #@param [\"None\", \"Blue\", \"Green\", \"Red\"]\n",
        "\n",
        "\n",
        "#@markdown ###Advanced Parameters\n",
        "\n",
        "Use_Default_Advanced_Parameters = False #@param {type:\"boolean\"}\n",
        "#@markdown ###If not, please input:\n",
        "learning_rate = 0.000001 #@param {type:\"number\"}\n",
        "weight_decay = 0.0001 #@param {type:\"number\"}\n",
        "\n",
        "if (Use_Default_Advanced_Parameters):\n",
        "  print(\"Default advanced parameters enabled\")\n",
        "  learning_rate = 0.1\n",
        "  weight_decay = 0.0001\n",
        "\n",
        "#here we check that no model with the same name already exist, if so delete\n",
        "model_path = train_dir + 'models/'\n",
        "if os.path.exists(model_path+'/'+model_name):\n",
        "  print(\"!! WARNING: \"+model_name+\" already exists and will be deleted in the following cell !!\")\n",
        "\n",
        "if len(test_dir) == 0:\n",
        "  test_dir = None\n",
        "\n",
        "# Here we match the channel to number\n",
        "if Channel_to_use_for_training == \"Grayscale\":\n",
        "  chan = 0\n",
        "elif Channel_to_use_for_training == \"Blue\":\n",
        "  chan = 3\n",
        "elif Channel_to_use_for_training == \"Green\":\n",
        "  chan = 2\n",
        "elif Channel_to_use_for_training == \"Red\":\n",
        "  chan = 1\n",
        "\n",
        "\n",
        "if Second_training_channel == \"Blue\":\n",
        "  chan2 = 3\n",
        "elif Second_training_channel == \"Green\":\n",
        "  chan2 = 2\n",
        "elif Second_training_channel == \"Red\":\n",
        "  chan2 = 1\n",
        "elif Second_training_channel == \"None\":\n",
        "  chan2 = 0\n",
        "\n",
        "if initial_model=='scratch':\n",
        "  initial_model = 'None'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8SDv9XztBgb"
      },
      "source": [
        "Here's what the command to train would be on the command line -- make sure if you run this locally to correct the paths for your local computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQsv-Iz7m_CF"
      },
      "outputs": [],
      "source": [
        "# run_str = f'python -m cellpose --use_gpu --verbose --train --dir {train_dir} --pretrained_model {initial_model} --chan {chan} --chan2 {chan2} --n_epochs {n_epochs} --learning_rate {learning_rate} --weight_decay {weight_decay}'\n",
        "# if test_dir is not None:\n",
        "#     run_str += f' --test_dir {test_dir}'\n",
        "# run_str += ' --mask_filter _seg.npy' # if you want to use _seg.npy files for training\n",
        "# print(run_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JRxBPmatrK7"
      },
      "source": [
        "## 2.3. Train new model\n",
        "\n",
        "Using settings from form above, train model in notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcYskYudMajM"
      },
      "outputs": [],
      "source": [
        "from cellpose import train\n",
        "\n",
        "# start logger (to see training across epochs)\n",
        "logger = io.logger_setup()\n",
        "\n",
        "# DEFINE CELLPOSE MODEL (without size model)\n",
        "model = models.CellposeModel(gpu=use_GPU, model_type=initial_model)\n",
        "\n",
        "# set channels\n",
        "channels = [chan, chan2]\n",
        "\n",
        "# get files\n",
        "output = io.load_train_test_data(train_dir, test_dir, mask_filter=\"_seg\")\n",
        "train_data, train_labels, _, test_data, test_labels, _ = output\n",
        "\n",
        "new_model_path = train.train_seg(model.net, train_data=train_data,\n",
        "                              train_labels=train_labels,\n",
        "                              test_data=test_data,\n",
        "                              test_labels=test_labels,\n",
        "                              channels=channels,\n",
        "                              save_path=train_dir,\n",
        "                              n_epochs=n_epochs,\n",
        "                              learning_rate=learning_rate,\n",
        "                              weight_decay=weight_decay,\n",
        "                              SGD=True,\n",
        "                              nimg_per_epoch=1,\n",
        "                              model_name=model_name,\n",
        "                              min_train_masks=1)\n",
        "\n",
        "# diameter of labels in training images\n",
        "# use model diameter if user diameter is 0\n",
        "diameter=0\n",
        "diameter = model.diam_labels if diameter==0 else diameter\n",
        "diam_labels = model.diam_labels.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdH0j8-L6FuB"
      },
      "source": [
        "## 2.4. Evaluate on test data (optional)\n",
        "\n",
        "If you have test data, check performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0AGsH5p6K6S"
      },
      "outputs": [],
      "source": [
        "# get files (during training, test_data is transformed so we will load it again)\n",
        "output = io.load_train_test_data(test_dir, mask_filter='_seg')\n",
        "test_data, test_labels = output[:2]\n",
        "# use model diameter if user diameter is 0\n",
        "\n",
        "# run model on test images\n",
        "masks = model.eval(test_data,\n",
        "                   channels=[chan, chan2],\n",
        "                   diameter=diam_labels)[0]\n",
        "\n",
        "# check performance using ground truth labels\n",
        "ap = metrics.average_precision(test_labels, masks)[0]\n",
        "print('')\n",
        "print(f'>>> average precision at iou threshold 0.5 = {ap[:,0].mean():.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8tZ8uYR-IFW"
      },
      "source": [
        "plot masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2ac5gtr-HPq"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,8), dpi=150)\n",
        "# use model diameter if user diameter is 0\n",
        "cols = 5 if len(test_data)>5 else len(test_data)\n",
        "for k,im in enumerate(test_data):\n",
        "    if k<cols:\n",
        "      img = im.copy()\n",
        "      plt.subplot(3,cols, k+1)\n",
        "      img = np.vstack((img, np.zeros_like(img)[:1]))\n",
        "      #img = img.transpose(1,2,0)\n",
        "      plt.imshow(img)\n",
        "      plt.axis('off')\n",
        "      if k==0:\n",
        "          plt.title('image')\n",
        "\n",
        "      plt.subplot(3,cols, cols + k+1)\n",
        "      plt.imshow(masks[k])\n",
        "      plt.axis('off')\n",
        "      if k==0:\n",
        "          plt.title('predicted labels')\n",
        "\n",
        "      plt.subplot(3,cols, 2*cols+ k+1)\n",
        "      plt.imshow(test_labels[k])\n",
        "      plt.axis('off')\n",
        "      if k==0:\n",
        "          plt.title('true labels')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbVIZbNk5hgR"
      },
      "source": [
        "# 3. Use custom model to segment images\n",
        "\n",
        "Take custom trained model from above, or upload your own model to google drive / colab runtime.\n",
        "\n",
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vDu4Ixjo588O"
      },
      "outputs": [],
      "source": [
        "# model name and path\n",
        "\n",
        "#@markdown ###Custom model path (full path):\n",
        "\n",
        "model_path = \"/content/train/models/CP_HPA_CrowdSourcing\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ###Path to images:\n",
        "\n",
        "dir = \"/content/test\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ###Channel Parameters:\n",
        "\n",
        "Channel_to_use_for_segmentation = \"Red\" #@param [\"Grayscale\", \"Blue\", \"Green\", \"Red\"]\n",
        "\n",
        "# @markdown If you have a secondary channel that can be used, for instance nuclei, choose it here:\n",
        "\n",
        "Second_segmentation_channel= \"Blue\" #@param [\"None\", \"Blue\", \"Green\", \"Red\"]\n",
        "\n",
        "\n",
        "# Here we match the channel to number\n",
        "if Channel_to_use_for_segmentation == \"Grayscale\":\n",
        "  chan = 0\n",
        "elif Channel_to_use_for_segmentation == \"Blue\":\n",
        "  chan = 3\n",
        "elif Channel_to_use_for_segmentation == \"Green\":\n",
        "  chan = 2\n",
        "elif Channel_to_use_for_segmentation == \"Red\":\n",
        "  chan = 1\n",
        "\n",
        "\n",
        "if Second_segmentation_channel == \"Blue\":\n",
        "  chan2 = 3\n",
        "elif Second_segmentation_channel == \"Green\":\n",
        "  chan2 = 2\n",
        "elif Second_segmentation_channel == \"Red\":\n",
        "  chan2 = 1\n",
        "elif Second_segmentation_channel == \"None\":\n",
        "  chan2 = 0\n",
        "\n",
        "#@markdown ### Segmentation parameters:\n",
        "\n",
        "#@markdown diameter of cells (set to zero to use diameter from training set):\n",
        "diameter =  0#@param {type:\"number\"}\n",
        "#@markdown threshold on flow error to accept a mask (set higher to get more cells, e.g. in range from (0.1, 3.0), OR set to 0.0 to turn off so no cells discarded):\n",
        "flow_threshold = 0.4 #@param {type:\"slider\", min:0.0, max:3.0, step:0.1}\n",
        "#@markdown threshold on cellprob output to seed cell masks (set lower to include more pixels or higher to include fewer, e.g. in range from (-6, 6)):\n",
        "cellprob_threshold=0 #@param {type:\"slider\", min:-6, max:6, step:1}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axg2YQEpDx0e"
      },
      "source": [
        "if you're using the example test data we'll copy it to a new folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InyKGtD3D2ZX"
      },
      "outputs": [],
      "source": [
        "src = 'human_in_the_loop/test'\n",
        "if dir[:len(src)] == src:\n",
        "    files = io.get_image_files(dir, '_masks')\n",
        "    dir = 'human_in_the_loop/eval/'\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "    for f in files:\n",
        "        dst = dir + os.path.split(f)[1]\n",
        "        print(f'{f} > {dst}')\n",
        "        shutil.copyfile(f, dst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JJ1q0nTBAAR"
      },
      "source": [
        "Here's what the command to train would be on the command line -- make sure if you run this locally to correct the paths for your local computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8P5voZOVM-H9"
      },
      "outputs": [],
      "source": [
        "run_str = f'python -m cellpose --use_gpu --verbose --dir {dir} --pretrained_model {model_path} --chan {chan} --chan2 {chan2} --diameter {diameter} --flow_threshold {flow_threshold} --cellprob_threshold {cellprob_threshold}'\n",
        "print(run_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN3rdsfMBc_8"
      },
      "source": [
        "## run custom model\n",
        "\n",
        "how to run the custom model in a notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCcbs722BYd0"
      },
      "outputs": [],
      "source": [
        "# gets image files in dir (ignoring image files ending in _masks)\n",
        "files = io.get_image_files(dir, '_masks')\n",
        "print(files)\n",
        "images = [io.imread(f) for f in files]\n",
        "\n",
        "# declare model\n",
        "model = models.CellposeModel(gpu=True,\n",
        "                             pretrained_model=model_path)\n",
        "\n",
        "# use model diameter if user diameter is 0\n",
        "diameter = model.diam_labels if diameter==0 else diameter\n",
        "\n",
        "# run model on test images\n",
        "masks, flows, styles = model.eval(images,\n",
        "                                  channels=[chan, chan2],\n",
        "                                  diameter=diameter,\n",
        "                                  flow_threshold=flow_threshold,\n",
        "                                  cellprob_threshold=cellprob_threshold\n",
        "                                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj5AIZ825o7P"
      },
      "source": [
        "## save output to *_seg.npy\n",
        "\n",
        "you will see the files save in the Files tab and you can download them from there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc7EWe_f5oEH"
      },
      "outputs": [],
      "source": [
        "from cellpose import io\n",
        "\n",
        "io.masks_flows_to_seg(images,\n",
        "                      masks,\n",
        "                      flows,\n",
        "                      files,\n",
        "                      channels=[chan, chan2],\n",
        "                      diams=diameter*np.ones(len(masks)),\n",
        "                      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwoUuuarC9V5"
      },
      "source": [
        "## save output masks to tiffs/pngs or txt files for imageJ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da-Rtx09DEZB"
      },
      "outputs": [],
      "source": [
        "io.save_masks(images,\n",
        "              masks,\n",
        "              flows,\n",
        "              files,\n",
        "              channels=[chan, chan2],\n",
        "              png=True, # save masks as PNGs and save example image\n",
        "              tif=True, # save masks as TIFFs\n",
        "              save_txt=True, # save txt outlines for ImageJ\n",
        "              save_flows=False, # save flows as TIFFs\n",
        "              save_outlines=False, # save outlines as TIFFs\n",
        "              save_mpl=True # make matplotlib fig to view (WARNING: SLOW W/ LARGE IMAGES)\n",
        "              )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiP9MWN4F3Sx"
      },
      "outputs": [],
      "source": [
        "f = files[0]\n",
        "plt.figure(figsize=(12,4), dpi=300)\n",
        "plt.imshow(io.imread(os.path.splitext(f)[0] + '_cp_output.png'))\n",
        "plt.axis('off')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
